#! /usr/bin/env python3
import os
import yaml
import logging
import re
import operator
import boto3

import xml.etree.ElementTree as ET
import urllib.request

import json

def main():
    logging.basicConfig(encoding='utf-8', level=logging.INFO)
    if 'LOGLEVEL' in os.environ:
        logging.getLogger().setLevel(os.environ['LOGLEVEL'])

    config_file = './feeds.yaml'
    if 'CONFIG_FILE' in os.environ:
        config_file = os.environ['CONFIG_FILE']

    config = {}
    with open(config_file, "r") as file:
        try:
            config = yaml.safe_load(file)
        except yaml.YAMLError as err:
            print(err)

    for feed in config['feeds']:
        logging.info(f"Processing feed '{feed['name']}' from '{feed['feed_url']}'")
        root = filter_rss_feed( feed )
        if 'merge_in' in feed:
            logging.debug(f"Merging")
            for url in feed['merge_in']:
                logging.debug(f"Merging in feed from '{url}'")
                merge_rss = ET.fromstring(download_file(url))
            root = merge_feeds(merge_rss, root)

        rss = ET.tostring(root, encoding='unicode', method='xml')
        write_feed(feed, rss)

# Merge source's episodes into dest's channel
# both should be ET objects
def merge_feeds(source, dest):
    #TODO: sort the items, though so far it seems podcast apps are happy to do that for me
    dest_channel = dest.find('channel')
    for item in source.iter(tag='item'):
        episode = get_episode_tags(item)
        if 'pubDate' in episode:
            logging.debug(f"Merging in '{episode['title']}' from '{episode['pubDate']}'")
            dest_channel.append(item)

    return dest

# Given a feed (a hash from the feeds.yaml file), downloads the RSS and uses
# ElementTree to be able to modify it.
# Then applies fiddles (which are feed-wide) and filters (which operate per-episode)
# Returns the ET representation of the feed
def filter_rss_feed(feed):
    rss = download_file(feed['feed_url'])
    root = ET.fromstring(rss)

    if 'fiddles' in feed:
        for fiddle in feed['fiddles']:
            fiddle_dispatch[fiddle['fiddle']]( root, fiddle['config'])

    # An 'item' is an episode in a 'channel' which is what you'd think of as a 'feed'
    channel = root.find('channel')
    for item in root.iter(tag='item'):
        episode = get_episode_tags(item)
        # A filter function returns 0 if it wants to remove the episode, 1 if it wants
        # to remove it. Default to keep
        if 'filters' in feed:
            keep = 1
            for filter in feed['filters']:
                keep = filter_dispatch[filter['filter']]( episode, filter['config'] )
                if keep < 1:
                    logging.debug(f"Removing '{episode['guid']}'")
                    channel.remove(item)

    return root

def get_episode_tags(item):
    episode = {}
    for field in item.findall('./'):
        tag=re.sub('{\S+}', '', field.tag)
        #TODO: Figure out what's going on here (rhlstp breaks it)
        #if tag in episode:
        #   logging.error(f"Tag '{tag}' already defined (as '{episode['tag']}') in feed")
        episode[tag] = field.text
    return episode

# Filter out episodes that are too short. This was triggered by the More or Less
# podcast where the World Service programme is a ~9min excerpt from the Radio 4
# one, and both are on the same feed
#
# Expects a threshold key in the config, one of 'minutes', 'seconds' or 'hours', and
# an operator (lessThan, lessThanOrEqualTo, etc.) to use to compare the duration with
# the threshold
def filter_duration(episode, config):

    logging.debug(f"Running shorter_than filter on '{episode['title']}' with config '{config}'");
    duration_seconds = -1
    if 'seconds' in config:
      duration_seconds = int(config['seconds'])
    elif 'minutes' in config:
      duration_seconds = int(config['minutes']) * 60
    elif 'hours' in config:
      duration_seconds = int(config['hours']) * 60 * 60

    if duration_seconds < 0:
        logging.debug("Could not parse duration out of value; aborting filter")
        logging.debug(f"Value: {config}")
        return 1

    operators = {
        'lessThan':             operator.lt,
        'lessThanOrEqualTo':    operator.le,
        'greaterThan':          operator.gt,
        'greaterThanOrEqualTo': operator.ge,
        'equalTo':              operator.eq,
        'notEqualTo':           operator.ne,
    }
    if not config['operator'] in operators:
        logging.error(f"Invalid operator '{config['operator']}")

    #TODO: Find other ways duration might be encoded
    if 'duration' in episode:
        episode_duration = int(episode['duration'])
        if operators[config['operator']]( episode_duration , duration_seconds):
            logging.debug(f"Filtering out '{episode['title']}'; duration of '{episode['duration']}' not '{config['operator']}' '{duration_seconds}' seconds)")
            return 0
    else:
        logging.debug(f"Retaining '{episode['title']}'; it has no 'itunes duration' field");
        return 0


    logging.debug(f"Retaining '{episode['title']}'; duration of '{episode['duration']}' greater than or equal to filter value of '{duration_seconds}' seconds ")
    return 1

# Filter out episodes with particular phrases in the fields. This was triggered by a
# few series doing a set of 'Book Club' episodes that are all about books I haven't read
#
# Config should have two keys, a 'pattern', and a 'field' to apply it to, and optionally
# a 'case_insensitive' which, if set, will cause the match to be case-insensitive
def filter_regex(episode, config):
    logging.debug(f"Running regex_exclude filter on '{episode['title']}' with config '{config}'")

    field_name = config['field']
    pattern = config['pattern']
    action = config['action']

    if not field_name in episode:
        logging.debug(f"Retaining '{episode['title']}'; it has no '{field_name}' field to match against")

    episode_matches = 0;

    if config['action'] == 'include':
        if re.search( pattern, episode[field_name] ):
            logging.debug(f"Retaining '{episode['title']}'; field '{field_name}' matches pattern '{pattern}'");
            return 1
        else:
            logging.debug(f"Filtering out '{episode['title']}'; field '{field_name}' does not match pattern '{pattern}'");
            return 0
    elif config['action'] == 'exclude':
        if re.search( pattern, episode[field_name] ):
            logging.debug(f"Filtering out '{episode['title']}'; field '{field_name}' matches pattern '{pattern}'");
            return 0
        else:
            logging.debug(f"Retaining '{episode['title']}'; field '{field_name}' does not match pattern '{pattern}'");
            return 1
    else:
        logging.error(f"Action '{config['action']}' invalid; must be either 'include' or 'exclude'")

    return 1

filter_dispatch = {
    'regex': filter_regex,
    'duration': filter_duration
}

# Testing this got confusing quickly as the new feeds had the same name, so
# this fiddle allows for changing the title
def fiddle_append_to_title(feed, config):
    logging.info(f"Appending '{config['string']}' to title")
    title = feed.find('channel/title')
    new_title = (f"{title.text} {config['string']}")
    title.text = new_title

    imagetitle = feed.find('channel/image/title')
    if imagetitle:
        new_title = (f"{imagetitle.text} {config['string']}")
        imagetitle.text = new_title

def fiddle_replace_title(feed, config):
    logging.info(f"Setting title to '{config['string']}'")
    title = feed.find('channel/title')
    title.text = config['string']

# Similarly, a new image might be useful!
def fiddle_replace_feed_image(feed, config):
    logging.info(f"Editing image to '{config['url']}'")
    image_url = feed.find('channel/image/url')
    if image_url:
        image_url.text = config['url']
    #TODO: Fixup images defined in other ways (like se2kb)

fiddle_dispatch = {
    'append_to_title': fiddle_append_to_title,
    'replace_title' : fiddle_replace_title,
    'replace_feed_image': fiddle_replace_feed_image
}

# # #
# #
#


def upload_file_s3(content, config):
    bucket_name = config['bucket']
    object_name = config['object']
    logging.info(f"S3 upload: writing object '{object_name}' to bucket '{bucket_name}'")
    s3 = boto3.resource('s3')
    s3.Object(bucket_name, object_name).put(Body=content, ACL='public-read')

def download_file(url):
    logging.debug(f"Downloading {url}")
    response = urllib.request.urlopen(url)
    data = response.read();
    content = data.decode('utf-8')
    return content

def write_feed(feed, rss):
    if 'file' in feed['output']:
        logging.info(f"Writing output feed to '{feed['output']['file']['path']}'")
        with open(feed['output']['file']['path'], "w") as f:
            print(rss, file=f)
    if 's3' in feed['output']:
        upload_file_s3(rss, feed['output']['s3'])


main()
